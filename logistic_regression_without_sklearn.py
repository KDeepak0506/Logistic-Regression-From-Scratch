# -*- coding: utf-8 -*-
"""Logistic Regression without sklearn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MdXVmUbQ6em7G-vXIZQbRWkBXLLrIPG-
"""

#importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#importing the dataset
df=pd.read_csv("/content/hours_vs_pass_custom.csv")
df

#Feature Extacting
x=np.array(df["Hours_Studied"])
y=np.array(df["Passed"])

#Visualizing the data
def DS_plot(x,y):
  sns.scatterplot(x=x,y=y,color="crimson")
  plt.title("Hours Studied vs Passed")
  plt.show()
DS_plot(x,y)

#Sigmoid function
def sigmoid(x):
  return (1/(1+np.exp(-x)))
#Model
def Logistic_Regression(x,y,learning_rate,epoch):
    #Initialization
    wights=0
    bias=0
    loss=[]
    m=len(y)
    #Training with Gradient descent
    for i in range(epoch):
      lin_f=wights*x+bias
      pred=sigmoid(lin_f)     #using Sigmoid function to convert the linear values to values between 0 and 1
      epsilon = 1e-15
      pred=np.clip(pred,epsilon,1-epsilon)      #using epsilon to restrict values like Log(0)
      BCE_loss=-(y*np.log(pred)+(1-y)*np.log(1-pred))     #Binary cross entropy loss
      loss.append(np.mean(BCE_loss))

      dw=(1/m)*np.dot(pred-y,x)
      db=(1/m)*np.sum(pred-y)

      wights=wights-learning_rate*dw      #Updating the wights and bias
      bias=bias-learning_rate*db

    plt.plot(loss)
    plt.title("Loss Curve")
    plt.show()

    return wights,bias,loss
weight,bias,loss=Logistic_Regression(x,y,0.01,10000)
print(np.mean(loss))
#Prediction
def CUS_PRED(x,weight,bias):
  lin_f=weight*x+bias
  pred=sigmoid(lin_f)
  return pred
pred=CUS_PRED(x,weight,bias)

DS_plot(x,pred)

#Classifiying based on predicted probabilities
class_pred=[]
for i in pred:
  if(i>=0.5):
    class_pred.append(1)
  else:
    class_pred.append(0)

#importing Logistic Regession
from sklearn.linear_model import LogisticRegression

#Training
def SKLOG(x,y):
  x_2d = x.reshape(-1, 1)
  y_2d = y.reshape(-1, 1)
  model_sk =LogisticRegression()
  model_sk.fit(x_2d,y_2d)
  y_pred_sk=model_sk.predict(x_2d)
  y_flat_sk=y_pred_sk.flatten()

  return y_flat_sk,model_sk,x_2d
y_flat_sk,model_sk,x_2d=SKLOG(x,y)

#Loss comparison of both Models
from sklearn.metrics import log_loss

def Model_accuracy(y, pred_probs, x_2d, model_sklearn):
    sklearn_probs = model_sklearn.predict_proba(x_2d)[:, 1]

    logloss_sklearn = log_loss(y, sklearn_probs)
    logloss_custom = log_loss(y, pred_probs)

    print(f"Sklearn Log Loss: {logloss_sklearn:.4f}")
    print(f"Custom Model Log Loss: {logloss_custom:.4f}")
Model_accuracy(y,pred,x_2d,model_sk)

#Generating a smooth range of x values (for curve plotting)
x_vals = np.linspace(min(x_2d)[0], max(x_2d)[0], 300).reshape(-1, 1)

sklearn_probs = model_sk.predict_proba(x_vals)[:, 1]
custom_probs = CUS_PRED(x_vals,weight,bias)

#Plotting actual data
plt.scatter(x_2d, y, color='green', label='Actual Data')

#Plotting sklearn model's curve
plt.plot(x_vals, sklearn_probs, color='blue', label='Sklearn Model', linewidth=2)

#Plotting custom model's curve
plt.plot(x_vals, custom_probs, color='crimson', label='Custom Model', linewidth=2)

plt.xlabel("Input Feature (e.g., Hours_Studied)")
plt.ylabel("Probability of Passing")
plt.title("Logistic Regression: Sklearn vs Custom")
plt.legend()
plt.ylim(-0.1, 1.1)
plt.show()

